{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf29623c",
   "metadata": {},
   "source": [
    "# Pruning + FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a1c7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\")\n",
    "sys.path.append(\"src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7f0cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\", \"../../src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "from torch.nn.utils import prune as nnprune\n",
    "import torch.nn as nn\n",
    "from engine import train_one_epoch, evaluate  # from cv-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75662fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.prune import should_skip_module, prune_convs_structured, prune_linears_unstructured\n",
    "from src.utils import Tee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d5a2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MODELS - https://github.com/pytorch/vision/tree/main/torchvision/models/detection\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    ssd300_vgg16, SSD300_VGG16_Weights,\n",
    "    ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSHead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af6d77",
   "metadata": {},
   "source": [
    "### Modeling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4764343",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/images_v1_v2'\n",
    "coco_path = '../data/annotations_v1_v2/coco_v1_v2.json'\n",
    "\n",
    "# 20% annotations\n",
    "aug_perc = 0.2\n",
    "sample_coco_path = f\"../data/annotations_v1_v2/coco_v1_v2_{aug_perc}.json\"\n",
    "\n",
    "image_size= (256, 256)  # (128, 128) (256, 256) (512, 512)\n",
    "batch_size = 2\n",
    "val_percent = 0.1\n",
    "num_classes =  2  # for just 1 object, classes will be 2 as background should be added as well\n",
    "\n",
    "# ---------------- Optim/training hyperparams -----------------------------------\n",
    "num_epochs = 12\n",
    "learning_rate = 0.0005\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 100\n",
    "\n",
    "# --- pruning hyperparams ---\n",
    "TARGET_SPARSITY_CONV = 0.10   # 30% channels per Conv (structured)\n",
    "TARGET_SPARSITY_FC   = 0.10   # 30% weights per Linear (unstructured)\n",
    "FINE_TUNE_EPOCHS     = 5\n",
    "FINE_TUNE_LR         = learning_rate * 0.2   # lower LR for recovery\n",
    "PRUNE_RPN_AND_ROI_PREDICTORS = False  # keep False for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a899e",
   "metadata": {},
   "source": [
    "# Apply Pruning\n",
    "### ðŸ”¹ Typical flow\n",
    "* Step\tPurpose\tLearning rate\n",
    "* Train full model\tLearn full capacity\tnormal (e.g., 1e-3)\n",
    "* Prune weights\tRemove least important connections\tâ€”\n",
    "* Fine-tune\tRestore accuracy on reduced model\tsmall (e.g., 1e-4)\n",
    "\n",
    "##### Without fine-tuning, your detectorâ€™s features become incoherent â†’ zero detections / zero AP (exactly what you saw).\n",
    "\n",
    "### ðŸ”¹ Analogy\n",
    "\n",
    "* Imagine removing 30 % of a musical instrumentâ€™s strings.\n",
    "* It can still produce sound, but itâ€™s out of tune.\n",
    "* Fine-tuning retrains the musician to play new chords with fewer strings â€” same music, less hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf5382",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trans = \"resize_horz_clrjtr_rot\"\n",
    "model_path = \"../models/frcnn_mobilenet_3/kd_resize_horz_clrjtr_rot_frcnn_mobilenet_epoch10.pth\"\n",
    "\n",
    "prune_model_dir = Path(f\"../models/Pruning\")\n",
    "prune_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = prune_model_dir / f\"prune_{trans}_log.txt\"\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device:\", device)\n",
    "\n",
    "print(f\"Will prune from: {model_path}\")\n",
    "\n",
    "# reload model\n",
    "prune_model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
    "in_feat = prune_model.roi_heads.box_predictor.cls_score.in_features\n",
    "prune_model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "prune_model.roi_heads.score_thresh = 0.3\n",
    "prune_model.load_state_dict(torch.load(model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "prune_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94839027",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conv_targets, linear_targets = [], []\n",
    "for name, m in prune_model.named_modules():\n",
    "    if should_skip_module(name): continue\n",
    "    if isinstance(m, nn.Conv2d):  conv_targets.append((m, name))\n",
    "    elif isinstance(m, nn.Linear): linear_targets.append((m, name))\n",
    "\n",
    "print(f\"[PRUNE] Conv2d: {len(conv_targets)} | Linear: {len(linear_targets)}\")\n",
    "\n",
    "prune_convs_structured(conv_targets, TARGET_SPARSITY_CONV)\n",
    "prune_linears_unstructured(linear_targets, TARGET_SPARSITY_FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aab64b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Make masks permanent\n",
    "for mod in prune_model.modules():\n",
    "    for attr in [\"weight\", \"bias\"]:\n",
    "        if hasattr(mod, f\"{attr}_mask\"):\n",
    "            try: nnprune.remove(mod, attr)\n",
    "            except: pass\n",
    "\n",
    "# Lower score threshold and evaluate\n",
    "prune_model.eval()\n",
    "prune_model.roi_heads.score_thresh = 0.3 #0.05\n",
    "evaluate(prune_model, data_loader_val, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab159f71",
   "metadata": {},
   "source": [
    "# Apply FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192ef25",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prune_model.train()\n",
    "ft_optim = torch.optim.SGD([p for p in prune_model.parameters() if p.requires_grad],\n",
    "                           lr=max(1e-5, learning_rate*0.2), momentum=momentum, weight_decay=weight_decay)\n",
    "ft_sched = torch.optim.lr_scheduler.StepLR(ft_optim, step_size=2, gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(prune_model, ft_optim, data_loader_train, device, epoch, print_freq=100)\n",
    "    ft_sched.step()\n",
    "    prune_model.eval()\n",
    "    prune_model.roi_heads.score_thresh = 0.3\n",
    "    evaluate(prune_model, data_loader_val, device=device)\n",
    "\n",
    "    # save model\n",
    "    model_name = f\"prune_{trans}_epoch{epoch}\"\n",
    "    model_save_path = prune_model_dir / f\"{model_name}.pth\"\n",
    "    print(\"model will be saved as:\", model_save_path, \"\\n\")\n",
    "    torch.save(prune_model.state_dict(), model_save_path)\n",
    "\n",
    "print(\"\\n[PRUNE] Final sparsity (pruned+finetuned):\")\n",
    "report_sparsity(prune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af31ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
