{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9fc71d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\")\n",
    "sys.path.append(\"src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\", \"../../src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch, re\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from packaging.version import Version\n",
    "\n",
    "import glob\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MODELS - https://github.com/pytorch/vision/tree/main/torchvision/models/detection\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    ssd300_vgg16, SSD300_VGG16_Weights,\n",
    "    ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.quant.fp16 import load_calib_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a64a04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/images_v1_v2'\n",
    "coco_path = '../data/annotations_v1_v2/coco_v1_v2.json'\n",
    "\n",
    "# 20% annotations\n",
    "aug_perc = 0.2\n",
    "sample_coco_path = f\"../data/annotations_v1_v2/coco_v1_v2_{aug_perc}.json\"\n",
    "\n",
    "image_size= (256, 256)  # (128, 128) (256, 256) (512, 512)\n",
    "batch_size = 2\n",
    "val_percent = 0.1\n",
    "num_classes =  2  # for just 1 object, classes will be 2 as background should be added as well\n",
    "\n",
    "# ---------------- Optim/training hyperparams -----------------------------------\n",
    "num_epochs = 12\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 100\n",
    "\n",
    "# ---------------- Quant hyperparams ----------------------------------------------\n",
    "calib_images_dir = \"../data/images_v1_v2\"  # <-- change to your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420395a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Your paths / hyperparams\n",
    "model_path = Path(\"../models/frcnn_mobilenet/frcnn_mobilenet_epoch10.pth\")\n",
    "num_classes = 2  # <-- set to your dataset (background + N objects)\n",
    "\n",
    "print(f\"\\033[91m{model_path}\\033[0m\")\n",
    "\n",
    "# Build base model\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=None)  # we load your trained head next\n",
    "in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "\n",
    "# Load weights\n",
    "state = torch.load(kd_horz_clrjtr_rot_model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "calib_imgs = load_calib_images(calib_images_dir, max_imgs=128)\n",
    "print(f\"Loaded {len(calib_imgs)} images for calibration.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[FP16] device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TORCH_VER = Version(re.sub(r'\\+.*$', '', torch.__version__))  # strip local build tags\n",
    "\n",
    "# 1) Import locations\n",
    "prepare_fx = convert_fx = prepare_qat_fx = None\n",
    "quantize_dynamic = None\n",
    "\n",
    "# FX prepare/convert moved around across versions:\n",
    "# - 1.8–1.12: torch.quantization.quantize_fx\n",
    "# - 1.13–2.0: torch.ao.quantization.quantize_fx\n",
    "# - 2.1+:     torch.ao.quantization.fx\n",
    "try:\n",
    "    from torch.ao.quantization.fx import prepare_fx, convert_fx, prepare_qat_fx\n",
    "except Exception:\n",
    "    try:\n",
    "        from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, prepare_qat_fx\n",
    "    except Exception:\n",
    "        from torch.quantization.quantize_fx import prepare_fx, convert_fx, prepare_qat_fx  # old path\n",
    "\n",
    "# Dynamic quant moved too:\n",
    "try:\n",
    "    from torch.ao.quantization import quantize_dynamic\n",
    "except Exception:\n",
    "    from torch.quantization import quantize_dynamic\n",
    "\n",
    "# 2) qconfig / qconfig_mapping differences\n",
    "# - Torch >= 2.1: get_default_qconfig_mapping(backend)\n",
    "# - Torch <= 2.0: use get_default_qconfig(backend) and pass a qconfig_dict={\"\": qconfig}\n",
    "qconfig_mapping = None\n",
    "qconfig_dict = None\n",
    "\n",
    "backend = \"qnnpack\" if torch.backends.quantized.engine in (None, \"\", \"qnnpack\") else torch.backends.quantized.engine\n",
    "# Prefer qnnpack on most CPUs; you can set \"fbgemm\" on AVX2/AVX512 Linux servers:\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "\n",
    "try:\n",
    "    from torch.ao.quantization import get_default_qconfig_mapping\n",
    "    qconfig_mapping = get_default_qconfig_mapping(torch.backends.quantized.engine)\n",
    "except Exception:\n",
    "    try:\n",
    "        # Older API\n",
    "        from torch.quantization import get_default_qconfig\n",
    "    except Exception:\n",
    "        from torch.ao.quantization import get_default_qconfig\n",
    "    qconfig = get_default_qconfig(torch.backends.quantized.engine)\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "\n",
    "def fx_prepare(module, example_inputs):\n",
    "    \"\"\"\n",
    "    Version-agnostic wrapper: returns 'prepared' module for PTQ or QAT depending on which\n",
    "    prepare_* you call outside.\n",
    "    \"\"\"\n",
    "    if qconfig_mapping is not None:\n",
    "        return prepare_fx(module, qconfig_mapping, example_inputs=(example_inputs,))\n",
    "    else:\n",
    "        # older API: prepare_fx(module, qconfig_dict, *), not mapping\n",
    "        return prepare_fx(module, qconfig_dict, example_inputs=(example_inputs,))\n",
    "\n",
    "def fx_convert(prepared):\n",
    "    return convert_fx(prepared)\n",
    "\n",
    "print(f\"[Quant FX Compat] torch=={torch.__version__} | engine={torch.backends.quantized.engine} | \"\n",
    "      f\"uses {'qconfig_mapping' if qconfig_mapping is not None else 'qconfig_dict'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 Extract the body we want to quantize (leave FPN + heads float)\n",
    "float_model = model  # original float model (eval)\n",
    "float_body = float_model.backbone.body     # MobileNetV3 features (Conv/BN/ReLU stacks)\n",
    "\n",
    "# 2.2 Build a wrapper to pass images through exactly like the backbone would\n",
    "class BodyWrapper(nn.Module):\n",
    "    def __init__(self, body: nn.Module):\n",
    "        super().__init__()\n",
    "        self.body = body\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: tensor of shape [N, 3, H, W] with float32 in [0,1]\n",
    "        return self.body(x)\n",
    "\n",
    "# Work on a *copy* so we don’t mutate your original model unless we succeed\n",
    "float_body_copy = copy.deepcopy(float_body).eval()\n",
    "\n",
    "# 2.3 Prepare FX graph for static PTQ\n",
    "# qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n",
    "example_batch = calib_imgs[0].unsqueeze(0) if len(calib_imgs) else torch.rand(1,3,640,640)  # fallback fake size\n",
    "\n",
    "# prepared = prepare_fx(float_body_copy, qconfig_mapping, example_inputs=(example_batch,))\n",
    "prepared = fx_prepare(float_body_copy, example_batch)\n",
    "prepared.eval()\n",
    "\n",
    "\n",
    "# 2.4 Calibration: run representative images through prepared module\n",
    "with torch.inference_mode():\n",
    "    for img in calib_imgs:\n",
    "        prepared(img.unsqueeze(0))\n",
    "\n",
    "# 2.5 Convert to quantized module\n",
    "# quantized_body = convert_fx(prepared)\n",
    "quantized_body = fx_convert(prepared)\n",
    "\n",
    "# 2.6 Reattach to the original model\n",
    "quantized_model = copy.deepcopy(float_model).eval()\n",
    "quantized_model.backbone.body = quantized_body\n",
    "quantized_model.eval()\n",
    "\n",
    "print(\"Static/PTQ: quantized backbone attached.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "quantized_model.roi_heads.box_head     = quantize_dynamic(quantized_model.roi_heads.box_head, {nn.Linear}, dtype=torch.qint8)\n",
    "quantized_model.roi_heads.box_predictor = quantize_dynamic(quantized_model.roi_heads.box_predictor, {nn.Linear}, dtype=torch.qint8)\n",
    "quantized_model.rpn.head.cls_logits    = quantize_dynamic(quantized_model.rpn.head.cls_logits, {nn.Linear}, dtype=torch.qint8)\n",
    "quantized_model.rpn.head.bbox_pred     = quantize_dynamic(quantized_model.rpn.head.bbox_pred,  {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "print(\"Dynamic: quantized ROI + RPN linear layers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_infer(m, imgs, warmup=3, iters=10):\n",
    "    m.eval()\n",
    "    with torch.inference_mode():\n",
    "        # warmup\n",
    "        for _ in range(warmup):\n",
    "            _ = m([imgs[0]])\n",
    "        # time\n",
    "        t0 = time.time()\n",
    "        for i in range(min(iters, len(imgs))):\n",
    "            _ = m([imgs[i]])\n",
    "        t1 = time.time()\n",
    "    return (t1 - t0)/min(iters, len(imgs))\n",
    "\n",
    "test_imgs = calib_imgs[:8] if len(calib_imgs) else [torch.rand(3,640,640) for _ in range(8)]\n",
    "\n",
    "# Baseline float (make a fresh copy so it’s not quantized)\n",
    "baseline_model = copy.deepcopy(model).eval()\n",
    "\n",
    "t_float = run_infer(baseline_model, test_imgs)\n",
    "t_quant = run_infer(quantized_model, test_imgs)\n",
    "\n",
    "print(f\"Avg CPU latency — float:  {t_float:.3f}s/img\")\n",
    "print(f\"Avg CPU latency — quant:  {t_quant:.3f}s/img\")\n",
    "\n",
    "# Quick structural check on one image\n",
    "with torch.inference_mode():\n",
    "    o_float = baseline_model([test_imgs[0]])[0]\n",
    "    o_quant = quantized_model([test_imgs[0]])[0]\n",
    "\n",
    "for k in [\"boxes\",\"scores\",\"labels\"]:\n",
    "    print(k, o_float[k].shape, \"->\", o_quant[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "save_dir = Path(\"../models/frcnn_mobilenet_quantized\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 5a. Try TorchScript (best for CPU-only inference deployment)\n",
    "scripted = torch.jit.script(quantized_model)\n",
    "torch.jit.save(scripted, save_dir / \"model_scripted_quant_1.pt\")\n",
    "print(\"Saved TorchScript quantized model:\", save_dir / \"model_scripted_quant.pt\")\n",
    "\n",
    "torch.save(quantized_model.state_dict(), save_dir / \"model_quantized_state_dict_1.pth\")\n",
    "print(\"Saved quantized state_dict:\", save_dir / \"model_quantized_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
