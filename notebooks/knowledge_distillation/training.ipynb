{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e09c835",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "\n",
    "For best results, these techniques are often applied sequentially:\n",
    "* Distillation: Train a lightweight Student model using a powerful Teacher model.18\n",
    "* Pruning: Further prune the Student model to reduce computation.\n",
    "* Quantization: Apply $\\text{INT}8$ quantization to the final pruned Student model for maximum deployment efficiency.\n",
    "\n",
    "* * * * *\n",
    "\n",
    "### ‚úÖ **Lightweight Models**\n",
    "\n",
    "Designed for mobile or resource-constrained environments:\n",
    "\n",
    "-   **`fasterrcnn_mobilenet_v3_large_fpn`**\n",
    "-   **`fasterrcnn_mobilenet_v3_large_320_fpn`** (lower resolution variant)\n",
    "-   **`ssdlite320_mobilenet_v3_large`** (SSD Lite optimized for mobile)\n",
    "-   **`ssd300_vgg16`** (still heavier than MobileNet but lighter than ResNet-based Faster R-CNN)\\\n",
    "    These models trade off some accuracy for speed and low memory footprint. Ideal for real-time inference on edge devices. [[deepwiki.com]](https://deepwiki.com/pytorch/vision/3.2-detection-models)\n",
    "\n",
    "* * * * *\n",
    "\n",
    "### ‚öñÔ∏è **Moderate Models**\n",
    "\n",
    "Balanced between accuracy and efficiency:\n",
    "\n",
    "-   **`retinanet_resnet50_fpn`**\n",
    "-   **`fcos_resnet50_fpn`**\\\n",
    "    These one-stage detectors are faster than two-stage models like Faster R-CNN but heavier than MobileNet-based SSD. Good for scenarios needing decent accuracy with reasonable latency. [[deepwiki.com]](https://deepwiki.com/pytorch/vision/3.2-detection-models)\n",
    "\n",
    "* * * * *\n",
    "\n",
    "### üîç **Heavy Models**\n",
    "\n",
    "High accuracy, large backbone, and higher compute requirements:\n",
    "\n",
    "-   **`fasterrcnn_resnet50_fpn`**\n",
    "-   **`fasterrcnn_resnet50_fpn_v2`** (improved accuracy)\n",
    "-   **`maskrcnn_resnet50_fpn`** (adds segmentation head ‚Üí even heavier)\n",
    "-   **`keypointrcnn_resnet50_fpn`** (adds keypoint detection head)\\\n",
    "    These two-stage detectors are best for high-accuracy tasks but require GPUs for real-time performance. [[deepwiki.com]](https://deepwiki.com/pytorch/vision/3.2-detection-models)\n",
    "\n",
    "* * * * *\n",
    "\n",
    "### **Rule of Thumb**\n",
    "\n",
    "-   **MobileNet-based ‚Üí Lightweight**\n",
    "-   **ResNet-50-based ‚Üí Moderate to Heavy**\n",
    "-   **Extra heads (Mask/Keypoint) ‚Üí Heavy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67175558",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\")\n",
    "sys.path.append(\"src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\", \"../../src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fpn_feature_hooks import FeatureHook, RPNHeadHook, FPNAdapter\n",
    "from src.student_model_builder import build_student\n",
    "from src.train import train_one_epoch_kd\n",
    "from src.utils import Tee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS - https://github.com/pytorch/vision/tree/main/torchvision/models/detection\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    ssd300_vgg16, SSD300_VGG16_Weights,\n",
    "    ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSHead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8469386",
   "metadata": {},
   "source": [
    "### Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/images_v1_v2'\n",
    "coco_path = '../data/annotations_v1_v2/coco_v1_v2.json'\n",
    "\n",
    "# 20% annotations\n",
    "aug_perc = 0.2\n",
    "sample_coco_path = f\"../data/annotations_v1_v2/coco_v1_v2_{aug_perc}.json\"\n",
    "\n",
    "image_size= (256, 256)  # (128, 128) (256, 256) (512, 512)\n",
    "batch_size = 2\n",
    "val_percent = 0.1\n",
    "num_classes =  2  # for just 1 object, classes will be 2 as background should be added as well\n",
    "\n",
    "# ---------------- Optim/training hyperparams -----------------------------------\n",
    "num_epochs = 12\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 100\n",
    "\n",
    "# ---------------- KD hyperparams ----------------------------------------------\n",
    "KD_T = 3.0                 # temperature for distillation\n",
    "LAMBDA_FPN = 0.5           # weight of FPN feature KD\n",
    "LAMBDA_RPN = 0.5           # weight of RPN objectness KD\n",
    "FEATURE_NORM = True        # L2-normalize along channels before MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = \"resize_horz_clrjtr_rot\"\n",
    "n_iter = 1\n",
    "student_variant = \"frcnn_mobilenet_320\"  # \"frcnn_mobilenet_320\"  # \"frcnn_mobilenet\" \n",
    "\n",
    "teacher_model_path = \"../models/fasterrcnn_resnet50_fpn_v2/20_resize_horz_clrjtr_rot_epoch7.pth\"\n",
    "\n",
    "\n",
    "KD_model_dir = Path(f\"../models/{student_variant}_{n_iter}\")\n",
    "KD_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = KD_model_dir / f\"kd_{trans}_log.txt\"\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eeec2f",
   "metadata": {},
   "source": [
    "data_loader_train, data_loader_val are generated using `CV-pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7348c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# applied_transforms = ResizeTransform(size=image_size)\n",
    "# dataset = CustomDataset(\n",
    "#     root=data_dir,\n",
    "#     annotation=coco_path,\n",
    "#     transforms=applied_transforms\n",
    "# )\n",
    "\n",
    "# # split\n",
    "# generator1 = torch.Generator().manual_seed(42)\n",
    "# val_size = int(val_percent * len(dataset))\n",
    "# train_size = len(dataset) - val_size\n",
    "# train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size], generator=generator1)\n",
    "\n",
    "# if \"clr\" in trans:\n",
    "#     aug_transforms = ResizeColorTransform(size=image_size)\n",
    "#     aug_dataset = CustomDataset(\n",
    "#         root=data_dir,\n",
    "#         annotation=sample_coco_path,  # augmentation subset (20%)\n",
    "#         transforms=aug_transforms\n",
    "#     )\n",
    "#     # concat augmented into train\n",
    "#     train_ds = ConcatDataset([train_ds, aug_dataset])\n",
    "#     print(f\"length of train dataset after applying ResizeColorTransform: {len(train_ds)}\")\n",
    "\n",
    "# if \"rot\" in trans:\n",
    "#     aug_transforms = ResizeRotateTransform(size=image_size)\n",
    "#     aug_dataset = CustomDataset(root=data_dir,\n",
    "#                                      annotation=sample_coco_path,\n",
    "#                                      transforms=aug_transforms)\n",
    "#     # Concat train and augmenented data\n",
    "#     train_ds = torch.utils.data.ConcatDataset([train_ds, aug_dataset])\n",
    "#     print(f\"length of train dataset after applying ResizeRotateTransform: {len(train_ds)}\")\n",
    "\n",
    "# if \"horz\" in trans:\n",
    "#     aug_transforms = ResizeHorzTransform(size=image_size)\n",
    "#     aug_dataset = CustomDataset(root=data_dir,\n",
    "#                                      annotation=sample_coco_path,  # augmentation subset (20%)\n",
    "#                                      transforms=aug_transforms)\n",
    "#     # Concat train and augmenented data\n",
    "#     train_ds = torch.utils.data.ConcatDataset([train_ds, aug_dataset])\n",
    "#     print(f\"length of train dataset after applying ResizeHorzTransform: {len(train_ds)}\")\n",
    "\n",
    "# data_loader_train = DataLoader(\n",
    "#     train_ds, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=utils.collate_fn\n",
    "# )\n",
    "# data_loader_val = DataLoader(\n",
    "#     val_ds, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=utils.collate_fn\n",
    "# )\n",
    "# print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188564e8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c4fa4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Teacher / Student builders\n",
    "# ==============================================================================\n",
    "def build_teacher(ckpt_path: Path, num_classes: int):\n",
    "    teacher = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "    in_feat = teacher.roi_heads.box_predictor.cls_score.in_features\n",
    "    teacher.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "    teacher.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    teacher.eval()\n",
    "    return teacher\n",
    "\n",
    "# Teacher\n",
    "teacher = build_teacher(teacher_model_path, num_classes=num_classes)\n",
    "\n",
    "# Student\n",
    "student = build_student(num_classes=num_classes, variant=student_variant)\n",
    "if hasattr(student, \"roi_heads\"):\n",
    "    student.roi_heads.score_thresh = 0.3  # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec618e53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Optim & per-iteration StepLR (decay every 3 epochs worth of iters)\n",
    "params = [p for p in student.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "iters_per_epoch = max(1, len(data_loader_train))\n",
    "step_size_iters = 3 * iters_per_epoch\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size_iters, gamma=0.1)\n",
    "\n",
    "# ---------------- Register hooks ----------------\n",
    "teacher_backbone_hook = FeatureHook()\n",
    "student_backbone_hook = FeatureHook()\n",
    "teacher_backbone_handle = teacher.backbone.register_forward_hook(teacher_backbone_hook)\n",
    "student_backbone_handle = student.backbone.register_forward_hook(student_backbone_hook)\n",
    "\n",
    "teacher_rpn_hook = RPNHeadHook()\n",
    "student_rpn_hook = RPNHeadHook()\n",
    "teacher_rpn_handle = teacher.rpn.head.register_forward_hook(teacher_rpn_hook)\n",
    "student_rpn_handle = student.rpn.head.register_forward_hook(student_rpn_hook)\n",
    "\n",
    "# FPN channel adapter\n",
    "adapter = FPNAdapter().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9806e26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "with open(log_path, \"w\") as f:\n",
    "    tee = Tee(sys.stdout, f)\n",
    "    with redirect_stdout(tee):\n",
    "        try:\n",
    "            for epoch in range(num_epochs):\n",
    "                stats = train_one_epoch_kd(\n",
    "                    teacher, student, optimizer, lr_scheduler,\n",
    "                    data_loader_train, device, epoch, KD_T, LAMBDA_FPN, LAMBDA_RPN, FEATURE_NORM,\n",
    "                    adapter, teacher_backbone_hook, student_backbone_hook,\n",
    "                    teacher_rpn_hook, student_rpn_hook,\n",
    "                    print_freq=print_freq\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                evaluate(student, data_loader_val, device=device)\n",
    "                \n",
    "                # Save\n",
    "                model_name = f\"kd_{trans}_epoch{epoch}\"\n",
    "                save_path = KD_model_dir / f\"{model_name}.pth\"\n",
    "                print(\"Saving:\", save_path)\n",
    "                torch.save(student.state_dict(), save_path)\n",
    "        finally:\n",
    "            # Clean up hooks\n",
    "            teacher_backbone_handle.remove()\n",
    "            student_backbone_handle.remove()\n",
    "            teacher_rpn_handle.remove()\n",
    "            student_rpn_handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e760de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e2081",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
