{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa076953",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\")\n",
    "sys.path.append(\"src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b2cf9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\", \"../../src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import glob\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d58cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MODELS - https://github.com/pytorch/vision/tree/main/torchvision/models/detection\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    ssd300_vgg16, SSD300_VGG16_Weights,\n",
    "    ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb4433",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.quant.fp16 import load_calib_images, norms_to_fp32, linears_to_half, run_quant_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b98b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/images_v1_v2'\n",
    "coco_path = '../data/annotations_v1_v2/coco_v1_v2.json'\n",
    "\n",
    "# 20% annotations\n",
    "aug_perc = 0.2\n",
    "sample_coco_path = f\"../data/annotations_v1_v2/coco_v1_v2_{aug_perc}.json\"\n",
    "\n",
    "image_size= (256, 256)  # (128, 128) (256, 256) (512, 512)\n",
    "batch_size = 2\n",
    "val_percent = 0.1\n",
    "num_classes =  2  # for just 1 object, classes will be 2 as background should be added as well\n",
    "\n",
    "# ---------------- Optim/training hyperparams -----------------------------------\n",
    "num_epochs = 12\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 100\n",
    "\n",
    "# ---------------- Quant hyperparams ----------------------------------------------\n",
    "calib_images_dir = \"../data/images_v1_v2\"  # <-- change to your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6756ab0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Your paths / hyperparams\n",
    "model_path = Path(\"../models/frcnn_mobilenet/frcnn_mobilenet_epoch10.pth\")\n",
    "num_classes = 2  # <-- set to your dataset (background + N objects)\n",
    "\n",
    "print(f\"\\033[91m{model_path}\\033[0m\")\n",
    "\n",
    "# Build base model\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=None)  # we load your trained head next\n",
    "in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "\n",
    "# Load weights\n",
    "state = torch.load(kd_horz_clrjtr_rot_model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35efea16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "calib_imgs = load_calib_images(calib_images_dir, max_imgs=128)\n",
    "print(f\"Loaded {len(calib_imgs)} images for calibration.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[FP16] device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1311f2",
   "metadata": {},
   "source": [
    "#### Option1\n",
    "backbone converted to half; norms kept in float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9714352",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Work on a copy so your original stays float32\n",
    "float_model = model.eval().to(device)\n",
    "fp16_model = copy.deepcopy(float_model).eval().to(device)\n",
    "\n",
    "# Convert the heavy conv backbone to half precision (weights + activations)\n",
    "fp16_model.backbone.body.half()\n",
    "\n",
    "norms_to_fp32(fp16_model.backbone.body)\n",
    "print(\"FP16: backbone converted to half; norms kept in float32.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061808c",
   "metadata": {},
   "source": [
    "#### Option2\n",
    "backbone + Linear in half, norms in float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267f807",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fp16_model = copy.deepcopy(model).eval().to(device)\n",
    "\n",
    "norms_to_fp32(fp16_model.backbone.body)\n",
    "\n",
    "linears_to_half(fp16_model.roi_heads)\n",
    "linears_to_half(fp16_model.rpn.head)\n",
    "\n",
    "print(\"FP16 ready: backbone + Linear in half, norms in float32.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7fc980",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inference uses autocast\n",
    "with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    _ = fp16_model([calib_imgs[0].to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576159f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_imgs = calib_imgs[:8] if len(calib_imgs) else [torch.rand(3,640,640) for _ in range(8)]\n",
    "\n",
    "baseline_model = copy.deepcopy(model).eval().to(device)\n",
    "t_float = run_quant_infer(baseline_model, test_imgs)\n",
    "t_fp16 = run_quant_infer(fp16_model, test_imgs)\n",
    "\n",
    "print(f\"Avg latency — float32: {t_float:.3f}s/img\")\n",
    "print(f\"Avg latency — fp16:    {t_fp16:.3f}s/img\")\n",
    "\n",
    "# Structural sanity check\n",
    "with torch.inference_mode():\n",
    "    img0 = test_imgs[0].to(device)\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(device.type=='cuda')):\n",
    "        o_float = baseline_model([img0])[0]\n",
    "        o_fp16  = fp16_model([img0])[0]\n",
    "\n",
    "for k in [\"boxes\",\"scores\",\"labels\"]:\n",
    "    print(k, o_float[k].shape, \"->\", o_fp16[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9b3fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "save_dir = Path(\"../models/frcnn_mobilenet_fp16\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "scripted = torch.jit.script(fp16_model.cpu().eval())  # script in CPU for portability\n",
    "torch.jit.save(scripted, save_dir / \"model_scripted_fp16.pt\")\n",
    "print(\"Saved TorchScript FP16 model:\", save_dir / \"model_scripted_fp16.pt\")\n",
    "\n",
    "# State dict preserves FP16 weights in the converted submodules\n",
    "torch.save(fp16_model.state_dict(), save_dir / \"model_fp16_state_dict.pth\")\n",
    "print(\"Saved FP16 state_dict:\", save_dir / \"model_fp16_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658f7ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63555f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
